{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic DataSet Kaggle Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Crisp DM Vorgehensmodel https://www.bigdata-insider.de/was-ist-crisp-dm-a-815478/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Geschäftsverständnis\n",
    "\n",
    "(engl. Business Understanding) In der Phase des Geschäftsverständnisses geht es darum, die konkreten Ziele und Anforderungen für das Data Mining festzulegen. Ergebnis dieser Phase ist die Formulierung der Aufgabenstellung und die Beschreibung der geplanten groben Vorgehensweise. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"visuals/titanic_sketch.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beschreibung der Titanic Story hier einfügen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle: https://de.wikipedia.org/wiki/RMS_Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: Timeline aus Kaggle kopieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition der Aufgabe: \"Vorhersage\" ob eine Passagier_In den Untergang der Titanic überlebt hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Datenverständnis \n",
    "\n",
    "(engl. Data Understanding) Im Rahmen des Datenverständnisses wird versucht, sich einen ersten Überblick über die zur Verfügung stehenden Daten und deren Qualität zu verschaffen. Es erfolgt eine Analyse und Bewertung der Datenqualität. Probleme mit der Qualität der vorhandenen Daten in Bezug auf die in der vorherigen Phase festgelegten Aufgabenstellung sind zu benennen. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vorbereitung der Entwicklungsumgebung\n",
    "Import der notwendigen Python Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd             # Pandas Bibliothek für Datenverarbeitung, siehe https://pandas.pydata.org/\n",
    "import matplotlib.pyplot as plt # Matplotlib für die Datenvisualisierung, siehe https://matplotlib.org/\n",
    "import seaborn as sns           # Seaborn für die Datenvisualisierung, siehe https://seaborn.pydata.org/\n",
    "import numpy as np              # Numpy für effiziente Rechenoperationen, siehe https://numpy.org/\n",
    "import os                       # OS für den Zugriff auf externe Daten, siehe https://docs.python.org/3/library/os.html\n",
    "\n",
    "\n",
    "# Sklearn ML-Algorithmen\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "\n",
    "# Sklearn Modellierungs-Helfer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, normalize, PowerTransformer\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatierung der Mark-up Tabellen: linksbündige Ausrichtung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Datenimport\n",
    "Import und merge der CSV Daten <br>\n",
    "Quelle: https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")                      # Trainingsdaten-Datei einlesen\n",
    "df_test  = pd.read_csv(\"data/test.csv\")                       # Testdaten-Datei einlesen\n",
    "df = pd.concat([df_train, df_test], ignore_index = True)      # beide Datensätze mergen, Trainingsdatensatz 0:892, Testdatensatz 892:1310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Datenerkundung\n",
    "Einen Überblick über die vorhandenen Daten verschaffen <br>\n",
    "Quellen: https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beschreibung der Daten\n",
    "\n",
    "| Variable | Bedeutung | Beschreibung |\n",
    "| :- | :- | :- |\n",
    "| PassengerId | Passagier_In-Nummer | Zahl zwischen 1 und 1309 |\n",
    "| Survived | Hat die Passagier_In den Untergang überlebt | 0 = Nein, 1 = Ja |\n",
    "| Pclass | Ticket-Klasse | 1 = 1. Klasse, 2 = 2. Klasse, 3 = 3. Klasse\n",
    "| Name | Name der Passagier_In | Nachname, Vorname |\n",
    "| Sex | Geschlecht der Passagier_In | male = Mann, female = Frau |\n",
    "| Age | Alter der Passagier_In |\n",
    "| SibSp | Anzahl der Geschwister/ Ehepartner an Board |\n",
    "| Parch | Anzahl der Kinder/ Eltern an Board |\n",
    "| Ticket | Ticket-Nummer |\n",
    "| Fare | Ticket-Preis |\n",
    "| Cabin | Kabinennummer |\n",
    "| Embarked | Einschiffungshafen | C = Cherbourg, Q = Queenstown, S = Southampton |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10) # Tabellarische Darstellung der ersten 10 Datensätze erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10) # Tabellarische Darstellung von 10 zufälligen Datensätze erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Beschreibung des gesamten Datensatzes: Fehlende Werte und Datentype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = \"all\") # Statistische Beschreibung des Datensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter der Pasagier_Innen\n",
    "sns.distplot(a = df[\"Age\"], bins = 16, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter der Pasagier_Innen\n",
    "sns.distplot(a = df[\"Fare\"], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter der Pasagier_Innen vs Überleben\n",
    "sns.catplot(x = \"Sex\", y = \"Survived\", kind = \"bar\", data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der Klassen\n",
    "sns.catplot(x = \"Pclass\", y = \"Survived\", kind = \"bar\", data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter vs Überleben\n",
    "sns.histplot(x = \"Age\", hue = \"Survived\", data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasse vs Überleben\n",
    "sns.displot(x = \"Pclass\", hue = \"Survived\", data = df, discrete = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Erkenntnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beschreibung der Daten\n",
    "\n",
    "| Variable | Bedeutung | Beschreibung |Formatierung fehlerhaft | Formatierung soll | Fehlende Werte |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| PassengerId | Passagier_In-Nummer | Zahl zwischen 1 und 1309 | ja | Integer | nein |\n",
    "| Survived | Hat die Passagier_In den Untergang überlebt | 0 = Nein, 1 = Ja | nein | | nein |\n",
    "| Pclass | Ticket-Klasse | 1 = 1. Klasse, 2 = 2. Klasse, 3 = 3. Klasse ja | Integer | | ja |\n",
    "| Name | Name der Passagier_In | Nachname, Vorname | nein | | nein | \n",
    "| Sex | Geschlecht der Passagier_In | male = Mann, female = Frau | nein | | ja |\n",
    "| Age | Alter der Passagier_In | | ja | Integer | ja |\n",
    "| SibSp | Anzahl der Geschwister/ Ehepartner an Board | | nein | | nein |\n",
    "| Parch | Anzahl der Kinder/ Eltern an Board | | nein | | nein |\n",
    "| Ticket | Ticket-Nummer | |  nein | | nein |\n",
    "| Fare | Ticket-Preis | |  nein | | ja |\n",
    "| Cabin | Kabinennummer | |  nein | | ja |\n",
    "| Embarked | Einschiffungshafen | C = Cherbourg, Q = Queenstown, S = Southampton |  nein | | ja |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Datenvorbereitung\n",
    "(engl. Data Preparation) Die Datenvorbereitung dient dazu, einen finalen Datensatz zu erstellen, der die Basis für die nächste Phase der Modellierung bildet. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Auffüllen der fehlenden Werte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"]      = df['Age'].fillna(df['Age'].median())            # Alter mit dem Median auffüllen\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0]) # Einschiffhafen mit dem Modus auffüllen\n",
    "df['Fare']     = df['Fare'].fillna(df['Fare'].median())          # Ticket-Preis mit dem Median auffüllen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Umwandlung der Eingangsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PassengerId\"] = df[\"PassengerId\"].astype(\"int\")              # Passagier_In-Nummer in Integer umwandeln\n",
    "#df[\"Survived\"]    = df[\"Survived\"].astype(\"boolean\")             # Überleben in Boolean umwandeln\n",
    "df[\"Age\"]         = df[\"Age\"].astype(\"int\")                      # Alter in Integer umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfung der Umwandlung\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Zusätzliche Features anlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Familiengröße\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "# Einzelreisende\n",
    "df['IsAlone']    = 1\n",
    "df['IsAlone'].loc[df['FamilySize'] > 1] = 0\n",
    "\n",
    "# Ticketkathegorie\n",
    "df['FareBin']    = pd.qcut(df['Fare'], 4)\n",
    "\n",
    "# Altersgruppe\n",
    "df['AgeBin']     = pd.cut(df['Age'].astype(int), 5)\n",
    "\n",
    "# Kabineninhaber_In\n",
    "df['CabinOwn']   = 1\n",
    "df['CabinOwn']   = df['CabinOwn'] - df['Cabin'].isna().astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Überflüssige Features löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen der Spalten Name, Cabin und Ticket\n",
    "df = df.drop([\"Name\", \"Cabin\", \"Ticket\"],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Encoding der Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label               = LabelEncoder()                      # Encoder Objekt anlegen\n",
    "df['Sex_Code']      = label.fit_transform(df['Sex'])      # Encodiertes Geschlecht\n",
    "df['Embarked_Code'] = label.fit_transform(df['Embarked']) # Encodierter Einschiffhafen\n",
    "df['AgeBin_Code']   = label.fit_transform(df['AgeBin'])   # Encodiertes Alter\n",
    "df['FareBin_Code']  = label.fit_transform(df['FareBin'])  # Encodierter Ticketpreis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Normalisierung und Standardisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_normed'] = (df['Age']-df['Age'].mean())/df['Age'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Dummy Variablen bilden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dummy = pd.get_dummies(df[\"Sex\", \"Pclass\", \"FamilySize\", \"Embarked\"]) \n",
    "#df_dummy_sex      = pd.get_dummies(df[\"Sex\"], prefix = \"sex\")\n",
    "#df_dummy_class    = pd.get_dummies(df[\"Pclass\"], prefix = \"Pclass\")\n",
    "#df_dummy_embarked = pd.get_dummies(df[\"Embarked\"], prefix = \"Embarked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([df, df_dummy_sex, df_dummy_class, df_dummy_embarked], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Aufteilung der Datensätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[0:891]\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test  = df[891:]\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Datenexploration durch Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter der Pasagier_Innen\n",
    "sns.histplot(x = \"Age\", hue = \"Sex\", data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = \"Sex\", y = \"Survived\", kind = \"bar\", data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = \"Pclass\", y = \"Survived\", kind = \"bar\", data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = \"AgeBin\", y = \"Survived\", kind = \"bar\", data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare vs Survive\n",
    "sns.catplot(x = \"FareBin\", y = \"Survived\", kind = \"bar\", data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair plots of entire dataset\n",
    "#pp = sns.pairplot(df_train, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "#pp.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Modellierung\n",
    "(engl. Modeling) <br> \n",
    "Im Rahmen der Modellierung werden die für die Aufgabenstellung geeigneten Methoden des Data Minings auf den in der Datenvorbereitung erstellten Datensatz angewandt. <br> \n",
    "Typisch für diese Phase sind die Optimierung der Parameter und die Erstellung mehrerer Modelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.1 Train - Test - Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"Fare\"]\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(df_train[params], df_train[\"Survived\"], random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_y.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.2 Training eines Logistischen Regressions Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialisieren und trainieren\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "log_reg = log_reg.fit(train1_x, train1_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model für Prognose verwenden und Prognose ausgeben\n",
    "log_reg_prediction = log_reg.predict(test1_x)\n",
    "print(\"Prognostizierte Werte\")\n",
    "print(log_reg_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahre Werte ausgeben\n",
    "test1_y_np = test1_y.to_numpy() # Pandas Serie in Numpy Array umwandeln\n",
    "print(\"Wahre Werte\")\n",
    "print(test1_y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich zwischen Prognose und wahren Werten ausgeben\n",
    "difference_pred_true = log_reg_prediction - test1_y_np\n",
    "print(\"Vergleich Prognose vs. Wahre Werte\")\n",
    "print(difference_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen der Vorhersagegenauigkeit\n",
    "count_zeros  = np.count_nonzero(log_reg_prediction-test1_y.to_numpy() == 0. , axis=0) # Nullen im Vergleichs Array zählen\n",
    "count_values = len(log_reg_prediction-test1_y.to_numpy())                             # Anzahl der Werte im Vergleichs Array zählen\n",
    "score        = count_zeros / count_values                                             # Anteil der korrekt vorhergesagten Werte zählen\n",
    "print(\"Genauigkeit des Models: %1.3f %%\"%(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Genauigkeit mit einer Zeile\n",
    "score = log_reg.score(test1_x, test1_y)\n",
    "print(\"Genauigkeit des Models: %1.3f %%\"%(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion für die Erstellung einer ansehnlichen Confusion Matrix\n",
    "# Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix erstellen\n",
    "conf_matrix = metrics.confusion_matrix(log_reg_prediction, test1_y.to_numpy())\n",
    "classes = [\"verstorben\", \"überlebt\"]\n",
    "plot_confusion_matrix(conf_matrix, classes, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.2 Training eines Logistischen Regressions Models mit mehreren Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"Fare\", \"Age\", \"FamilySize\"]\n",
    "train2_x, test2_x, train2_y, test2_y = model_selection.train_test_split(df_train[params], df_train[\"Survived\"], random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialisieren und trainieren\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "log_reg = log_reg.fit(train2_x, train2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model für Prognose verwenden und Prognose ausgeben\n",
    "log_reg_prediction = log_reg.predict(test2_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Genauigkeit mit einer Zeile\n",
    "score = log_reg.score(test2_x, test2_y)\n",
    "print(\"Genauigkeit des Models: %1.3f %%\"%(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix erstellen\n",
    "conf_matrix = metrics.confusion_matrix(log_reg_prediction, test2_y.to_numpy())\n",
    "classes = [\"verstorben\", \"überlebt\"]\n",
    "plot_confusion_matrix(conf_matrix, classes, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.3 Vergleich mit dem Zufall (Münzwurf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zufälliges Vorhersage erstellen\n",
    "random_prediction = np.random.randint(0,2,size = len(test2_y)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahre Werte ausgeben\n",
    "test2_y_np = test2_y.to_numpy() # Pandas Serie in Numpy Array umwandeln\n",
    "\n",
    "# Vergleich zwischen Zufall und wahren Werten ausgeben\n",
    "difference_pred_true = random_prediction - test2_y_np\n",
    "\n",
    "# Berechnen der Vorhersagegenauigkeit\n",
    "count_zeros  = np.count_nonzero(random_prediction - test2_y.to_numpy() == 0. , axis=0) # Nullen im Vergleichs Array zählen\n",
    "count_values = len(random_prediction - test2_y.to_numpy())                             # Anzahl der Werte im Vergleichs Array zählen\n",
    "score        = count_zeros / count_values                                              # Anteil der korrekt vorhergesagten Werte zählen\n",
    "print(\"Genauigkeit des Models: %1.3f %%\"%(score))\n",
    "\n",
    "# Confusion Matrix erstellen\n",
    "conf_matrix = metrics.confusion_matrix(random_prediction, test2_y.to_numpy())\n",
    "classes = [\"verstorben\", \"überlebt\"]\n",
    "plot_confusion_matrix(conf_matrix, classes, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.4 Training eines Support Vector Machine Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterdefinition\n",
    "params = [\"CabinOwn\", \"Sex_Code\", \"Embarked_Code\", \"AgeBin_Code\", \"FareBin_Code\"]\n",
    "\n",
    "# Train-Test-Split\n",
    "train4_x, test4_x, train4_y, test4_y = model_selection.train_test_split(df_train[params], df_train[\"Survived\"], random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialisieren, trainieren und Vorhersage erstellen\n",
    "svm = svm.SVC(decision_function_shape='ovo')\n",
    "svm = svm.fit(train4_x, train4_y)\n",
    "svm_prediction = svm.predict(test4_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model score\n",
    "score = svm.score(test4_x, test4_y)\n",
    "print(\"Genauigkeit des Models: %1.3f %%\"%(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix erstellen\n",
    "conf_matrix = metrics.confusion_matrix(svm_prediction, test4_y.to_numpy())\n",
    "classes = [\"verstorben\", \"überlebt\"]\n",
    "plot_confusion_matrix(conf_matrix, classes, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.5 Training eines k-Next-Neighbors Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterdefinition\n",
    "params = [\"CabinOwn\", \"Sex_Code\", \"Embarked_Code\", \"AgeBin_Code\", \"FareBin_Code\"]\n",
    "\n",
    "# Train-Test-Split\n",
    "train5_x, test5_x, train5_y, test5_y = model_selection.train_test_split(df_train[params], df_train[\"Survived\"], random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialisieren, trainieren und Vorhersage erstellen\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn = knn.fit(train5_x, train5_y)\n",
    "knn_prediction = knn.predict(test5_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model score\n",
    "score = knn.score(test5_x, test5_y)\n",
    "print(\"Genauigkeit des Models: %1.3f %%\"%(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix erstellen\n",
    "conf_matrix = metrics.confusion_matrix(knn_prediction, test5_y.to_numpy())\n",
    "classes = [\"verstorben\", \"überlebt\"]\n",
    "plot_confusion_matrix(conf_matrix, classes, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.6 Training eines Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterdefinition\n",
    "params = [\"CabinOwn\", \"Sex_Code\", \"Embarked_Code\", \"AgeBin_Code\", \"FareBin_Code\"]\n",
    "\n",
    "# Train-Test-Split\n",
    "train6_x, test6_x, train6_y, test6_y = model_selection.train_test_split(df_train[params], df_train[\"Survived\"], random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialisieren, trainieren und Vorhersage erstellen\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators = 20, max_depth = 4)\n",
    "rfc = rfc.fit(train6_x, train6_y)\n",
    "rfc_predicition = rfc.predict(test6_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model score\n",
    "score = rfc.score(test6_x, test6_y)\n",
    "print(\"Genauigkeit des Models: %1.3f %%\"%(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix erstellen\n",
    "conf_matrix = metrics.confusion_matrix(rfc_predicition, test6_y.to_numpy())\n",
    "classes = [\"verstorben\", \"überlebt\"]\n",
    "plot_confusion_matrix(conf_matrix, classes, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.7 Kombination der Vorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(data = {\"log_reg\" : log_reg.predict(train2_x),\n",
    "                                \"svm\" : svm.predict(train4_x), \n",
    "                                \"kNN\" : knn.predict(train5_x),\n",
    "                                \"rfc\" : rfc.predict(train6_x), \n",
    "                                \"true\" : train1_y.to_numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\"log_reg\", \"kNN\", \"svm\", \"rfc\", \"true\"]\n",
    "for feature in feature_list:\n",
    "    features[feature] = features[feature].astype(bool)\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterdefinition\n",
    "params = [\"log_reg\", \"kNN\", \"svm\", \"rfc\"]\n",
    "\n",
    "# Train-Test-Split\n",
    "train0_x, test0_x, train0_y, test0_y = model_selection.train_test_split(features[params], features[\"true\"], random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialisieren, trainieren und Vorhersage erstellen\n",
    "ensemble_model = ensemble.RandomForestClassifier(n_estimators = 3, max_depth = 3, criterion = \"entropy\")\n",
    "ensemble_model= ensemble_model.fit(train0_x, train0_y)\n",
    "ensemble_model_prediction = ensemble_model.predict(test0_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model score\n",
    "score = ensemble_model.score(test0_x, test0_y)\n",
    "print(\"Genauigkeit des Models: %1.3f %%\"%(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix erstellen\n",
    "conf_matrix = metrics.confusion_matrix(ensemble_model_prediction, test0_y.to_numpy())\n",
    "classes = [\"verstorben\", \"überlebt\"]\n",
    "plot_confusion_matrix(conf_matrix, classes, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract single tree\n",
    "estimator = ensemble_model.estimators_[0]\n",
    "\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = params,\n",
    "                class_names = [\"verstorben\", \"überlebt\"],\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 1, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract single tree\n",
    "estimator = ensemble_model.estimators_[1]\n",
    "\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = params,\n",
    "                class_names = [\"verstorben\", \"überlebt\"],\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 1, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Vorhersage auf ungesehene Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten laden\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_continous = [\"Fare\", \"Age\", \"FamilySize\"]\n",
    "params_classes = [\"CabinOwn\", \"Sex_Code\", \"Embarked_Code\", \"AgeBin_Code\", \"FareBin_Code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_preds = pd.DataFrame(log_reg.predict(df_test[params_continous]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_preds = pd.DataFrame(data = {\"log_reg\" : log_reg.predict(df_test[params_continous]).astype(int),\n",
    "                                     \"svm\"     : svm.predict(df_test[params_classes]).astype(int), \n",
    "                                     \"kNN\"     : knn.predict(df_test[params_classes]).astype(int),\n",
    "                                     \"rfc\"     : rfc.predict(df_test[params_classes]).astype(int)})\n",
    "df_test_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_preds[\"ensemble\"] = ensemble_model.predict(df_test_preds).astype(int)\n",
    "df_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = pd.DataFrame(data = {\"PassengerId\" : df_test[\"PassengerId\"].values, \n",
    "                              \"Survived\" : df_test_preds[\"ensemble\"].astype(int)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
